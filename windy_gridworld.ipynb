{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "windy_gridworld.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpZh2X5iTQa62TSaJeBrr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/ReinforcementLearning/blob/main/windy_gridworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cyefJ_J9lTWg"
      },
      "outputs": [],
      "source": [
        "from pickletools import UP_TO_NEWLINE\n",
        "from stat import UF_OPAQUE\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "np.random.seed(4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the actions\n",
        "# For this environment, we will do math operations, so it is better to define the actions as numpy\n",
        "UP = np.array([0,1])\n",
        "DOWN = np.array([0,-1])\n",
        "RIGHT = np.array([1,0])\n",
        "LEFT = np.array([-1,0])\n",
        "\n",
        "\n",
        "ACTIONS  = [ UP, DOWN, RIGHT, LEFT]\n",
        "ACTION_TO_TEXT={tuple(UP):'UP', tuple(DOWN):'DOWN', tuple(LEFT):'LEFT', tuple(RIGHT):'RIGHT'}\n",
        "\n",
        "class StochWindyGridWorldEnv(gym.Env):\n",
        "    \n",
        "    def __init__(self, GRID_HEIGHT=7, GRID_WIDTH=10,\n",
        "                 WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0], \n",
        "                 START_CELL = np.array([0, 3]), \n",
        "                 GOAL_CELL = np.array([7, 4]),\n",
        "                 REWARD = -1, \n",
        "                 RANGE_RANDOM_WIND=2):\n",
        "        self.grid_height = GRID_HEIGHT\n",
        "        self.grid_width = GRID_WIDTH\n",
        "        self.grid_dimensions = (self.grid_height, self.grid_width)\n",
        "        self.wind = np.array(WIND)\n",
        "        self.start_cell = START_CELL\n",
        "        self.goal_cell = GOAL_CELL        \n",
        "        self.reward = REWARD\n",
        "        self.range_random_wind = RANGE_RANDOM_WIND\n",
        "        self.action_space = gym.spaces.Discrete(len(ACTIONS))\n",
        "        self.observation_space = gym.spaces.MultiDiscrete((self.grid_width, self.grid_height))\n",
        "        self.state = self.start_cell\n",
        "        self.max_runs = 1000\n",
        "        self.n_runs = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_cell\n",
        "        self.n_runs = 0\n",
        "        return self.state\n",
        "\n",
        "    # Get the observation based on our current state\n",
        "    # This function is simple here, but may be more complex depending on\n",
        "    # the task\n",
        "    # Returns:\n",
        "    #   obs: the observation of the current state\n",
        "    def _get_obs(self):\n",
        "        return self.state\n",
        "\n",
        "    # This function returns the next state ocurring in response to an action in the previous\n",
        "    # state. \n",
        "    def transition(self, action, state): \n",
        "        # First we add the action and wind\n",
        "        next_state = state + action\n",
        "        next_state[1] += self.wind[state[0]]\n",
        "        # Next we clip to make sure we are in the grid\n",
        "        next_state[0] =  min(max(next_state[0], 0),self.grid_width-1)\n",
        "        next_state[1] =  min(max(next_state[1], 0),self.grid_height-1)\n",
        "        return next_state\n",
        "\n",
        "    # This function progresses the environment one timestep given the current\n",
        "    # state and the action. This is where the dynamics are applied.\n",
        "    # Inputs:\n",
        "    #   action: The desired action to apply to the environment\n",
        "    # Returns:\n",
        "    #   obs: observation of the new state\n",
        "    #   reward: the reward received for the transition\n",
        "    #   done: a variable indicating whether we have terminated or not\n",
        "    #   info: a dictionary data structure containing additional information\n",
        "    #         about the environment we may want to track\n",
        "    def step(self, action):\n",
        "        self.n_runs = self.n_runs + 1\n",
        "        act = ACTIONS[action]\n",
        "        # Get our current state so we can calculate the reward later\n",
        "        state = self._get_obs()\n",
        "\n",
        "        # We have no additional information to pass back now\n",
        "        info = {}\n",
        "\n",
        "        # Get the entry for this state-action pair in our transition table\n",
        "        next_state = self.transition(act, state)\n",
        "\n",
        "        done = True if np.array_equal(next_state,self.goal_cell) or (self.n_runs > self.max_runs) else False \n",
        "\n",
        "        reward = -1 \n",
        "\n",
        "        # We make sure to update our current state\n",
        "        self.state = next_state\n",
        "        return self._get_obs(), reward, done, info \n",
        "\n",
        "    def render(self, draw_policy = False, draw_state = True):\n",
        "        plt.ion()\n",
        "        plt.clf()\n",
        "        plt.xlim(0, self.grid_width)\n",
        "        plt.ylim(0, self.grid_height)\n",
        "        \n",
        "        for y in range(self.grid_height):\n",
        "            for x in range(self.grid_width):\n",
        "                #print(\"{:12.4f}\".format(v[i, j]), end=' ')\n",
        "                if(draw_policy):\n",
        "                    a = ACTIONS[np.random.randint(0,3)]\n",
        "                    plt.arrow(x+0.5,y+0.5,a[1]*0.3,-a[0]*0.3,head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
        "           #print('\\n')\n",
        "        if(draw_state):\n",
        "            plt.plot(self.goal_cell[0]+0.5, self.goal_cell[1]+0.5, 'b*', markersize=12)\n",
        "            plt.plot(self.state[0]+0.5, self.state[1]+0.5, 'r.', markersize=12)\n",
        "\n",
        "        plt.gca().set_xticks(np.arange(0, self.grid_width, 1))\n",
        "        plt.gca().set_yticks(np.arange(0, self.grid_height, 1))\n",
        "        plt.grid(True,fillstyle = 'full')\n",
        "        plt.tick_params(\n",
        "        axis='both',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom=False,      # ticks along the bottom edge are off\n",
        "        top=False,         # ticks along the top edge are off\n",
        "        left = False,\n",
        "        right = False,\n",
        "        labelbottom=False,\n",
        "        labelleft = False) # labels along the bottom edge are off\n",
        "        for i in range(0, len(self.wind)):\n",
        "            plt.text(0.4 + i, -0.5, self.wind[i], fontsize=16)\n",
        "        plt.draw()\n",
        "        plt.pause(0.0001)"
      ],
      "metadata": {
        "id": "3SJQR5x3lTz6"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}