{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "environment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObkWHdbP7LhWbh6IleqFF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/ReinforcementLearning/blob/main/environment_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nk5WD0OQ7Xz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.special as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "class MazeEnvironment:    \n",
        "    def __init__(self, maze, init_position, goal):\n",
        "        x = len(maze)\n",
        "        y = len(maze)\n",
        "        \n",
        "        self.boundary = np.asarray([x, y])\n",
        "        self.init_position = init_position\n",
        "        self.current_position = np.asarray(init_position)\n",
        "        self.goal = goal\n",
        "        self.maze = maze\n",
        "        \n",
        "        self.visited = set()\n",
        "        self.visited.add(tuple(self.current_position))\n",
        "                \n",
        "        # initialize the empty cells and the euclidean distance from\n",
        "        # the goal (removing the goal cell itself)\n",
        "        self.allowed_states = np.asarray(np.where(self.maze == 0)).T.tolist()\n",
        "        self.distances = np.sqrt(np.sum((np.array(self.allowed_states) -\n",
        "                                         np.asarray(self.goal))**2,\n",
        "                                         axis = 1))\n",
        "        \n",
        "        del(self.allowed_states[np.where(self.distances == 0)[0][0]])\n",
        "        self.distances = np.delete(self.distances, np.where(self.distances == 0)[0][0])\n",
        "                \n",
        "        self.action_map = {0: [0, 1],\n",
        "                           1: [0, -1],\n",
        "                           2: [1, 0],\n",
        "                           3: [-1, 0]}\n",
        "        \n",
        "        self.directions = {0: '→',\n",
        "                           1: '←',\n",
        "                           2: '↓ ',\n",
        "                           3: '↑'}\n",
        "        \n",
        "        # the agent makes an action from the following:\n",
        "        # 1 -> right, 2 -> left\n",
        "        # 3 -> down, 4 -> up\n",
        "        \n",
        "    # introduce a reset policy, so that for high epsilon the initial\n",
        "    # position is nearer to the goal (useful for large mazes)\n",
        "    def reset_policy(self, eps, reg = 7):\n",
        "        return sp.softmax(-self.distances/(reg*(1-eps**(2/reg)))**(reg/2)).squeeze()\n",
        "    \n",
        "    # reset the environment when the game is completed\n",
        "    # with probability prand the reset is random, otherwise\n",
        "    # the reset policy at the given epsilon is used\n",
        "    def reset(self, epsilon, prand = 0):\n",
        "        if np.random.rand() < prand:\n",
        "            idx = np.random.choice(len(self.allowed_states))\n",
        "        else:\n",
        "            p = self.reset_policy(epsilon)\n",
        "            idx = np.random.choice(len(self.allowed_states), p = p)\n",
        "\n",
        "        self.current_position = np.asarray(self.allowed_states[idx])\n",
        "        \n",
        "        self.visited = set()\n",
        "        self.visited.add(tuple(self.current_position))\n",
        "\n",
        "        return self.state()\n",
        "    \n",
        "    \n",
        "    def state_update(self, action):\n",
        "        isgameon = True\n",
        "        \n",
        "        # each move costs -0.05\n",
        "        reward = -0.05\n",
        "        \n",
        "        move = self.action_map[action]\n",
        "        next_position = self.current_position + np.asarray(move)\n",
        "        \n",
        "        # if the goals has been reached, the reward is 1\n",
        "        if (self.current_position == self.goal).all():\n",
        "                reward = 1\n",
        "                isgameon = False\n",
        "                return [self.state(), reward, isgameon]\n",
        "            \n",
        "        # if the cell has been visited before, the reward is -0.2\n",
        "        else:\n",
        "            if tuple(self.current_position) in self.visited:\n",
        "                reward = -0.2\n",
        "        \n",
        "        # if the moves goes out of the maze or to a wall, the\n",
        "        # reward is -1\n",
        "        if self.is_state_valid(next_position):\n",
        "            self.current_position = next_position\n",
        "        else:\n",
        "            reward = -1\n",
        "        \n",
        "        self.visited.add(tuple(self.current_position))\n",
        "        return [self.state(), reward, isgameon]\n",
        "\n",
        "    # return the state to be feeded to the network\n",
        "    def state(self):\n",
        "        state = copy.deepcopy(self.maze)\n",
        "        state[tuple(self.current_position)] = 2\n",
        "        return state\n",
        "        \n",
        "    \n",
        "    def check_boundaries(self, position):\n",
        "        out = len([num for num in position if num < 0])\n",
        "        out += len([num for num in (self.boundary - np.asarray(position)) if num <= 0])\n",
        "        return out > 0\n",
        "    \n",
        "    \n",
        "    def check_walls(self, position):\n",
        "        return self.maze[tuple(position)] == 1\n",
        "    \n",
        "    \n",
        "    def is_state_valid(self, next_position):\n",
        "        if self.check_boundaries(next_position):\n",
        "            return False\n",
        "        elif self.check_walls(next_position):\n",
        "            return False\n",
        "        return True\n",
        "    \n",
        "    \n",
        "    def draw(self, filename):\n",
        "        plt.figure()\n",
        "        im = plt.imshow(self.maze, interpolation='none', aspect='equal', cmap='Greys');\n",
        "        ax = plt.gca();\n",
        "\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "\n",
        "        ax.plot(self.goal[1], self.goal[0],\n",
        "                'bs', markersize = 4)\n",
        "        ax.plot(self.current_position[1], self.current_position[0],\n",
        "                'rs', markersize = 4)\n",
        "        plt.savefig(filename, dpi = 300, bbox_inches = 'tight')\n",
        "        plt.show()"
      ]
    }
  ]
}