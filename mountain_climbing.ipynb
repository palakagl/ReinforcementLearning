{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mountain_climbing_Palak_Agrawal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOcDkXIloYp2ESmHLqgG8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/ReinforcementLearning/blob/main/mountain_climbing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XhyQWj7E-JZ",
        "outputId": "9ae67dbe-6cc0-4ab1-e824-1881b26c9a98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting gym\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[K     |████████████████████████████████| 626 kB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.11.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Collecting gym-notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.6-py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym) (4.2.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701374 sha256=4390085332b3492ef15f226263e814a2ed23b64558bd4816f4aab63303b9ed1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/33/04/6723848e46f0f1ebe794bb329b7c761c3329a0d7ffade99da7\n",
            "Successfully built gym\n",
            "Installing collected packages: gym-notices, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.23.1 gym-notices-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(4)"
      ],
      "metadata": {
        "id": "ZRJ7BvnZAZ1t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our actions\n",
        "CLIMB=0\n",
        "PREPARE=1\n",
        "ACTIONS=[CLIMB,PREPARE]\n",
        "\n",
        "# Define our states\n",
        "BASE_CAMP=0\n",
        "BASE_CAMP_PREPARED=1\n",
        "GLACIER=2\n",
        "GLACIER_PREPARED=3\n",
        "ICE_WALL=4\n",
        "ICE_WALL_PREPARED=5\n",
        "PEAK=6\n",
        "\n",
        "# We define a mapping of actions and states for readability later\n",
        "ACTION_TO_TEXT={CLIMB:'CLIMB', PREPARE:'PREPARE'}\n",
        "STATE_TO_TEXT={BASE_CAMP:'BASE_CAMP', BASE_CAMP_PREPARED: 'BASE_CAMP_PREPARED', GLACIER: 'GLACIER', GLACIER_PREPARED: 'GLACIER_PREPARED', \\\n",
        "               ICE_WALL: 'ICE_WALL', ICE_WALL_PREPARED: 'ICE_WALL_PREPARED', PEAK: 'PEAK'}"
      ],
      "metadata": {
        "id": "d6X3Haj9IUGm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ9x5bQ7AVZU",
        "outputId": "c701dd78-e898-4781-a756-6b6ea3979691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: BASE_CAMP, Action: CLIMB, State': BASE_CAMP, Reward: -5\n",
            "State: BASE_CAMP, Action: CLIMB, State': GLACIER, Reward: 3\n",
            "State: GLACIER, Action: PREPARE, State': GLACIER_PREPARED, Reward: -1\n",
            "State: GLACIER_PREPARED, Action: CLIMB, State': ICE_WALL, Reward: 3\n",
            "State: ICE_WALL, Action: PREPARE, State': ICE_WALL_PREPARED, Reward: -1\n",
            "State: ICE_WALL_PREPARED, Action: CLIMB, State': PEAK, Reward: 3\n"
          ]
        }
      ],
      "source": [
        "# An OpenAI Gym code skeleton of the mountain climbing MDP for A2\n",
        "# Developed for MMAI-845\n",
        "class mountainClimbing(gym.Env):\n",
        "    def __init__(self,\n",
        "            ):\n",
        "\n",
        "        # We set the number of states internally\n",
        "        self.num_states = 7\n",
        "\n",
        "        # We must set the size of our observations and actions so an agent\n",
        "        # can be created for the environment\n",
        "        self.observation_space = gym.spaces.Discrete(self.num_states)\n",
        "        self.action_space = gym.spaces.Discrete(len(ACTIONS))\n",
        "        \n",
        "        # Task A.1\n",
        "        # INSERT CODE HERE\n",
        "        # Fill in the correct start state to reset into\n",
        "        self.init_state = BASE_CAMP \n",
        "\n",
        "        # Create an empty table to hold our transition probabilities\n",
        "        self.transition_table = {}\n",
        "        \n",
        "        # Task A.2\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out the correct values for the probabilities for each state (Switch the None values to the right values)\n",
        "        # Remember that each probability must add to 1\n",
        "        # All of the entries for the table must be filled\n",
        "        self.transition_table[BASE_CAMP] = {\n",
        "                CLIMB: {GLACIER: 0.9, BASE_CAMP: 0.1},\n",
        "                PREPARE: {BASE_CAMP_PREPARED: 1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[BASE_CAMP_PREPARED] = {\n",
        "                CLIMB: {GLACIER: 1},\n",
        "                }\n",
        "\n",
        "        self.transition_table[GLACIER] = {\n",
        "                CLIMB: {ICE_WALL: 0.3, GLACIER: 0.7},\n",
        "                PREPARE: {GLACIER_PREPARED: 1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[GLACIER_PREPARED] = {\n",
        "                CLIMB: {ICE_WALL: 1},\n",
        "                }\n",
        "\n",
        "        self.transition_table[ICE_WALL] = {\n",
        "                CLIMB: {PEAK: 0.7, ICE_WALL: 0.3},\n",
        "                PREPARE: {ICE_WALL_PREPARED: 1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[ICE_WALL_PREPARED] = {\n",
        "                CLIMB: {PEAK: 1},\n",
        "                }\n",
        "\n",
        "        # We define a fixed reward table based on the possible transitions\n",
        "        # It is possible to calculate this purely on the transition without\n",
        "        # predefining this table as well\n",
        "        # Task A.3\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out this table with the correct rewards from the MDP for every possible transition (Switch the None values to the right values)\n",
        "        self.reward_table = {\n",
        "                (BASE_CAMP, CLIMB, BASE_CAMP): -5,\n",
        "                (BASE_CAMP, CLIMB, GLACIER): 3,\n",
        "                (BASE_CAMP, PREPARE, BASE_CAMP_PREPARED): -1,\n",
        "                (BASE_CAMP_PREPARED, CLIMB, GLACIER): 3,\n",
        "                (GLACIER, CLIMB, GLACIER): -5,\n",
        "                (GLACIER, CLIMB, ICE_WALL): 3,\n",
        "                (GLACIER, PREPARE, GLACIER_PREPARED): -1,\n",
        "                (GLACIER_PREPARED, CLIMB, ICE_WALL): 3,\n",
        "                (ICE_WALL, CLIMB, ICE_WALL): -5,\n",
        "                (ICE_WALL, CLIMB, PEAK): 3,\n",
        "                (ICE_WALL, PREPARE, ICE_WALL_PREPARED): -1,\n",
        "                (ICE_WALL_PREPARED, CLIMB, PEAK): 3,\n",
        "                }\n",
        "\n",
        "    # Place us in the initial state\n",
        "    # This does not need to be deterministic\n",
        "    # Returns:\n",
        "    #   obs: an observation of our current state after the reset\n",
        "    def reset(self):\n",
        "        self.state = self.init_state\n",
        "        return self._get_obs()\n",
        "\n",
        "    # Get the observation based on our current state\n",
        "    # This function is simple here, but may be more complex depending on\n",
        "    # the task\n",
        "    # Returns:\n",
        "    #   obs: the observation of the current state\n",
        "    def _get_obs(self):\n",
        "        return self.state\n",
        "\n",
        "    # This function progresses the environment one timestep given the current\n",
        "    # state and the action. This is where the dynamics are applied.\n",
        "    # Inputs:\n",
        "    #   action: The desired action to apply to the environment\n",
        "    # Returns:\n",
        "    #   obs: observation of the new state\n",
        "    #   reward: the reward received for the transition\n",
        "    #   done: a variable indicating whether we have terminated or not\n",
        "    #   info: a dictionary data structure containing additional information\n",
        "    #         about the environment we may want to track\n",
        "    def step(self, action):\n",
        "        # Get our current state so we can calculate the reward later\n",
        "        state = self._get_obs()\n",
        "\n",
        "        # We have no additional information to pass back now\n",
        "        info = {}\n",
        "\n",
        "        # Get the entry for this state-action pair in our transition table\n",
        "        transition_entry = self.transition_table[state][action]\n",
        "\n",
        "        # Since we have a low number of fixed states, we can process the entry\n",
        "        # into states and probabilities easily directly. With a more complex\n",
        "        # table, we can iterate over the transitions\n",
        "        possible_states = list(transition_entry.keys())\n",
        "        state_probabilities = list(transition_entry.values())\n",
        "\n",
        "        # We use the numpy library to select the next state according to our\n",
        "        # probability distribution\n",
        "        next_state = np.random.choice(possible_states, p=state_probabilities)\n",
        "        \n",
        "        # Task B.1\n",
        "        # INSERT CODE HERE\n",
        "        # Enter the correct state on which we terminate here\n",
        "        done = True if next_state==PEAK else False \n",
        "\n",
        "        # Task B.2\n",
        "        # INSERT CODE HERE\n",
        "        # Call the reward function for the transition correctly\n",
        "        # Hint: use the self. prefix to call functions in the object we are in\n",
        "        reward = self._get_reward(state, action, next_state) \n",
        "\n",
        "        # We make sure to update our current state\n",
        "        self.state = next_state\n",
        "        return self._get_obs(), reward, done, info \n",
        "\n",
        "    # This function calculates the reward for a given_transition\n",
        "    # Inputs:\n",
        "    #   state: The current state\n",
        "    #   action: The action applied\n",
        "    #   next_state: The next state we enter\n",
        "    # Returns:\n",
        "    #   reward: The given reward for the transitions\n",
        "    def _get_reward(self, state, action, next_state):\n",
        "        index = (state, action, next_state)\n",
        "        return self.reward_table[index]\n",
        "\n",
        "# This class will output an action for each state according to the specificatin given in the environment\n",
        "class policy():\n",
        "    # Nothing to do for initialization in this case\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # This function take the state and returns the correct action\n",
        "    # Inputs:\n",
        "    #   state: an integer representing the state\n",
        "    # Output:\n",
        "    #   action: an integer representing the action specified in the assignmnet text\n",
        "    def __call__(self, state):\n",
        "        action = None\n",
        "        # Q2\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out the correct state in the empty lists in the if statements, and attach the correct actions\n",
        "        # You may add more if/elif statements if you wish, but only two are necessary \n",
        "        if state in [BASE_CAMP, BASE_CAMP_PREPARED ,GLACIER_PREPARED, ICE_WALL_PREPARED]:\n",
        "            action = CLIMB\n",
        "        elif state in [GLACIER, ICE_WALL]:\n",
        "            action = PREPARE\n",
        "\n",
        "        return action\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # NOTE: Set the flag below to 0 if you want to debug only the environment code, and 1 if you want to check on the policy\n",
        "    # Keep in mind that if the value is set to zero, only the climb action is taken, so there may still be an error in the \n",
        "    # prepare actions or states which this would not reveal\n",
        "    debug_flag=1\n",
        "\n",
        "    env = mountainClimbing()\n",
        "    # We need to reset the environment to initialize it\n",
        "    state = env.reset()\n",
        "    if debug_flag==0: \n",
        "        done = False\n",
        "        while not done:\n",
        "            # Always select the climb action in this debug mode \n",
        "            action = CLIMB \n",
        "            # We apply our action and observe the outcome\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # We print the transition and reward for visualization\n",
        "            print(\"State: {}, Action: {}, State': {}, Reward: {}\".format(\\\n",
        "                    STATE_TO_TEXT[state], ACTION_TO_TEXT[action], STATE_TO_TEXT[next_state], reward))\n",
        "            state = next_state\n",
        "\n",
        "    elif debug_flag==1:\n",
        "        pol = policy()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Call our policy to get the action for this state\n",
        "            action =  pol(state) \n",
        "            # We apply our action and observe the outcome\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # We print the transition and reward for visualization\n",
        "            print(\"State: {}, Action: {}, State': {}, Reward: {}\".format(\\\n",
        "                    STATE_TO_TEXT[state], ACTION_TO_TEXT[action], STATE_TO_TEXT[next_state], reward))\n",
        "            state = next_state"
      ]
    }
  ]
}