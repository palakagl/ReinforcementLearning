{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tabular.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4s1OlZ+o3EB9nJGjPPMHE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/ReinforcementLearning/blob/main/tabular_windy_gridworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n3h133KilE11"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to select an action based off of Q vals\n",
        "# with epsilon greedy exploration\n",
        "# Takes table entry for Q(s, *) and epsilon\n",
        "# Return action index\n",
        "def epsilon_greedy_action(q_table_entry, epsilon):\n",
        "    num_acts = len(q_table_entry)\n",
        "    probs = np.zeros((num_acts))\n",
        "    # Choose the greedy action\n",
        "    ind = np.argmax(q_table_entry)\n",
        "    # Remove epsilon from the greedy action probability\n",
        "    probs[ind] = 1.0 - epsilon\n",
        "    # Distribute epsilon among all actions\n",
        "    probs += epsilon/num_acts\n",
        "    # Choose our action\n",
        "    return np.random.choice(num_acts, p=probs)\n",
        "\n",
        "# Tabular Sarsa implementation\n",
        "def sarsa(env, step_size=0.5, epsilon=0.1, gamma=1.0, num_eps=100):\n",
        "\n",
        "    # Initialize rewards and our q_table. \n",
        "    # The dimensions of this table are: number of columns x number of rows x number of actions  \n",
        "    q_table = np.zeros((*env.observation_space.nvec, env.action_space.n))\n",
        "    reward_list = []\n",
        "    \n",
        "    # Loop for each episode:\n",
        "    for ep in range(num_eps):\n",
        "        # Reset everything before starting a new episode. Initialize S\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_list.append(0)\n",
        "\n",
        "        # Select an epsilon greedy action. (Choose A from S using policy derived from Q (e.g., epsilon-greedy))\n",
        "        action = epsilon_greedy_action(q_table[state[0], state[1],:], epsilon)\n",
        "\n",
        "        #Loop for each step of episode\n",
        "        while not done:\n",
        "            # Get our new state. (Take action A, observe R, S_prime)\n",
        "            state_prime, rew, done, info = env.step(action)\n",
        "\n",
        "            # Choose the action of our next state (Choose A_prime from S_prime using policy derived from Q (e.g., epsilon-greedy))\n",
        "            action_prime = epsilon_greedy_action(q_table[state_prime[0], state_prime[1],:], epsilon)\n",
        "\n",
        "            # Get the relevant Q values for our action in this state and the next\n",
        "            q = q_table[state[0], state[1], action]\n",
        "\n",
        "            q_prime = q_table[state_prime[0], state_prime[1], action_prime]\n",
        "\n",
        "            # List of acronyms and useful information\n",
        "            # Q(S,A) -> q_table[state[0],state[1], action], which is also q\n",
        "            # alpha -> step_size\n",
        "            # R -> rew\n",
        "            # Q(s',a') -> q_table[state_prime[0], state_prime[1], action_prime]\n",
        "            # End of list of acronyms and useful information\n",
        "\n",
        "            # Task 1.1 - Update the table according to SARSA\n",
        "            # INSERT CODE HERE\n",
        "            # Q(s,a) = Q(s_a) + alpha * (r(s,a) + gamma * Q(s',a') - Q(s,a))\n",
        "            q_table[state[0], state[1], action] = q_table[state[0], state[1], action] + step_size * (rew + (gamma * (q_table[state_prime[0], state_prime[1], action_prime] - q_table[state[0], state[1], action])))\n",
        "\n",
        "            # Update our current state and action\n",
        "            state = state_prime\n",
        "            action = action_prime\n",
        "            reward_list[ep] += rew\n",
        "\n",
        "    return reward_list, q_table\n",
        "\n",
        "# Tabular Q-learning implementation\n",
        "def q_learning(env, step_size=0.5, epsilon=0.1, gamma=1.0, num_eps=100):\n",
        "    # Initialize rewards and our q_table\n",
        "    q_table = np.zeros((*env.observation_space.nvec, env.action_space.n))\n",
        "    reward_list = []\n",
        "\n",
        "    # Loop for each episode:\n",
        "    for ep in range(num_eps):\n",
        "        # Reset everything before starting a new episode\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_list.append(0)\n",
        "        \n",
        "        # Loop for each step of episode:\n",
        "        while not done:\n",
        "            # Select an epsilon greedy action (Choose A from S using policy derived from Q (e.g., epsilon-greedy))\n",
        "            action = epsilon_greedy_action(q_table[state[0], state[1],:], epsilon)\n",
        "\n",
        "            # Progress the environment (Take action A, observe R, S_prime)\n",
        "            state_prime, rew, done, info = env.step(action)\n",
        "\n",
        "            # Get the q_values we need\n",
        "            q = q_table[state[0], state[1], action]\n",
        "\n",
        "\n",
        "            # Task 2.1 - Select the best action, which is the one with the maximum Q values (This is the max_a Q(S',a) in the equation) of q_all_actions\n",
        "            q_all_actions = q_table[state_prime[0], state_prime[1], :]\n",
        "            # INSERT CODE HERE\n",
        "            q_prime = max(q_all_actions)\n",
        "\n",
        "            # List of acronyms and useful information\n",
        "            # Q(S,A) -> q_table[state[0],state[1], action], which is also q\n",
        "            # alpha -> step_size\n",
        "            # R -> rew\n",
        "            # Q(s',a') -> q_table[state_prime[0], state_prime[1], action_prime]\n",
        "            # End of list of acronyms and useful information\n",
        "\n",
        "            # Task 2.2 - Update the table according to Q-learning\n",
        "            # INSERT CODE HERE\n",
        "            # Q(s,a) = Q(s_a) + alpha * (r(s,a) + gamma * max(Q(s',*)) - Q(s,a))\n",
        "            q_table[state[0], state[1], action] = q_table[state[0], state[1], action] + step_size * (rew + (gamma * max(q_table[state_prime[0], state_prime[1], :] - q_table[state[0], state[1], action])))\n",
        "\n",
        "            # Update state and rewards\n",
        "            state = state_prime\n",
        "            reward_list[ep] += rew\n",
        "\n",
        "    return reward_list, q_table\n"
      ],
      "metadata": {
        "id": "BJShBll4lGq8"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}