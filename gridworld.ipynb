{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridworld_palak_agrawal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWW3ZBy7F9s+oGwMNOJuJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/ReinforcementLearning/blob/main/gridworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "metadata": {
        "id": "tYEJZlFsCRZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr1ms23zs5al",
        "outputId": "c6805565-e397-4aeb-b092-d30ebc4e7218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Policy Evaluation\n",
            "\t3.31 \t8.79 \t4.43 \t5.33 \t1.5 \n",
            "\t1.53 \t3.0 \t2.25 \t1.91 \t0.55 \n",
            "\t0.05 \t0.74 \t0.68 \t0.36 \t-0.4 \n",
            "\t-0.97 \t-0.43 \t-0.35 \t-0.58 \t-1.18 \n",
            "\t-1.85 \t-1.34 \t-1.23 \t-1.42 \t-1.97 \n",
            "-----------------------------------------------------\n",
            "Running Value Iteration\n",
            "\t21.97 \t24.42 \t21.98 \t19.42 \t17.48 \n",
            "\t19.78 \t21.98 \t19.78 \t17.8 \t16.02 \n",
            "\t17.8 \t19.78 \t17.8 \t16.02 \t14.42 \n",
            "\t16.02 \t17.8 \t16.02 \t14.42 \t12.98 \n",
            "\t14.42 \t16.02 \t14.42 \t12.98 \t11.68 \n",
            "\n",
            "Optimal policy after computing the optimal values\n",
            "\tE \tE \tW \tE \tW \n",
            "\tE \tN \tW \tW \tW \n",
            "\tE \tN \tW \tW \tW \n",
            "\tE \tN \tW \tW \tW \n",
            "\tE \tN \tW \tW \tW \n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Defining the special states with higher rewards\n",
        "GOAL_STATES = [(1,0), (3, 0)]\n",
        "\n",
        "# Defining the transition from the special states\n",
        "GOAL_NEXT_STATES = {\n",
        "        (1,0): (1,4),\n",
        "        (3,0): (3,2),\n",
        "        }\n",
        "\n",
        "# Defining the special rewards\n",
        "TRANSITION_REWARDS = {\n",
        "        ((1,0), (1,4)): 10.0,\n",
        "        ((3,0), (3,2)): 5.0\n",
        "        }\n",
        "\n",
        "# Defining the actions\n",
        "ACTIONS  = [\n",
        "        (1,0),  #EAST, increase column by 1\n",
        "        (-1,0), #WEST, decrease column by 1\n",
        "        (0,-1),  #NORTH, decrease row by 1\n",
        "        (0,1)  #SOUTH, increase row by 1\n",
        "         ]\n",
        "\n",
        "# Utility function to print the grid in an ordered way\n",
        "def print_grid(grid, dim=5):\n",
        "    for j in range(dim):\n",
        "        row_str = ''\n",
        "        for i in range(dim):\n",
        "            row_str += '\\t{} '.format(np.round(grid[i,j], 2))\n",
        "        print(row_str)\n",
        "\n",
        "\n",
        "# Utility function to print the policy in an ordered way\n",
        "def print_policy(policy, dim=5):\n",
        "    pol_enum = {(1,0): 'E',\n",
        "            (-1,0): 'W',\n",
        "            (0,-1): 'N',\n",
        "            (0,1): 'S',}\n",
        "    for j in range(dim):\n",
        "        row_str = ''\n",
        "        for i in range(dim):\n",
        "            row_str += '\\t{} '.format(pol_enum[policy[(i,j)]])\n",
        "        print(row_str)\n",
        "\n",
        "# An OpenAI Gym environment for the gridworld described in A3\n",
        "# Developed for MMAI-845\n",
        "# Note the two functions which we allow our agent to access, transition and get_rew\n",
        "# These functions allow us to use dynamic programming, as the agent has knowledge of the environment\n",
        "# dynamics\n",
        "# Also note that we don't actually set initial states or transitions, since we don't need to actually\n",
        "# interact with the environment with a perfect (known) model\n",
        "class gridworld(gym.Env):\n",
        "    def __init__(self,\n",
        "            grid_dim=5,\n",
        "            ):\n",
        "\n",
        "        self.grid_dim = grid_dim \n",
        "        # We must set the size of our observations and actions so an agent\n",
        "        # can be created for the environment\n",
        "        self.observation_space = gym.spaces.MultiDiscrete((self.grid_dim, self.grid_dim))\n",
        "        self.action_space = gym.spaces.Discrete(len(ACTIONS))\n",
        "\n",
        "    # Place us in the initial state\n",
        "    # This does not need to be deterministic\n",
        "    # Returns:\n",
        "    #   obs: an observation of our current state after the reset\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    # This function returns the next state ocurring in response to an action in the previous\n",
        "    # state. This is used to update the values in DP.\n",
        "    def transition(self, state, action):        \n",
        "        # First, we update the state if we are not in a goal state\n",
        "        if state not in GOAL_STATES:\n",
        "            # Apply the action\n",
        "            next_state = (state[0] + action[0], state[1] + action[1]) \n",
        "            # We modify the value here to make sure we are still in the grid\n",
        "            next_x = max(min(next_state[0], self.grid_dim - 1), 0)\n",
        "            next_y = max(min(next_state[1], self.grid_dim - 1), 0)\n",
        "            next_state = (next_x, next_y)\n",
        "        # If we are in the goal state, we move to a fixed state regardless of the action\n",
        "        else:\n",
        "            next_state = GOAL_NEXT_STATES[state]\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        # Our default reward is 0\n",
        "        reward = 0\n",
        "        # If we are in a goal state before acting, we change the reward based on the problem definition\n",
        "        if state in GOAL_STATES:\n",
        "            reward = TRANSITION_REWARDS[(state, next_state)]\n",
        "        # If we hit a wall, our reward is -1\n",
        "        if state == next_state:\n",
        "            reward = -1\n",
        "        return reward\n",
        "\n",
        "    def step(self, action):\n",
        "        pass\n",
        "\n",
        "# We define a simple random policy for the assignment.\n",
        "# This chooses each action with probability 0.25 for each state\n",
        "class assignmentUniformPolicy():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # Return a list of the actions and probabilities\n",
        "    def __call__(self, state):\n",
        "        action_probabilities = {}\n",
        "        for action in ACTIONS:\n",
        "            action_probabilities[action] = 1.0 / len(ACTIONS)\n",
        "        return action_probabilities \n",
        "\n",
        "# This function implements the policy evaluation algorithm\n",
        "# It returns a table of the values for each state\n",
        "def policy_evaluation(environment, tolerance, policy, gamma):\n",
        "    # We create a list of zeros of dimensions nxn, where n is the dimensionality of the grid\n",
        "    # Note that this could be randomly instantiated, as per the algorithm states\n",
        "    value_table = np.zeros((environment.grid_dim, environment.grid_dim)) \n",
        "\n",
        "    # Set the delta high so we enter the loop initially\n",
        "    delta = float('inf')\n",
        "\n",
        "    # We loop until the difference in values is smaller than the tolerance we define\n",
        "    while delta > tolerance:\n",
        "        delta = 0 \n",
        "        # Since our state is a grid, we loop through every combination of x and y position\n",
        "        # First is the x position\n",
        "        for i in range(environment.grid_dim):\n",
        "            # Second is the y position\n",
        "            for j in range(environment.grid_dim):\n",
        "                # Store our value to check the delta\n",
        "                value_old = value_table[i, j]\n",
        "                state = (i,j)\n",
        "                value = 0\n",
        "\n",
        "                # Loop through every action to find the overall value\n",
        "                for action in ACTIONS:\n",
        "\n",
        "                    # Get the actions and its probabilities given the state\n",
        "                    actions_probabilities = policy(state) \n",
        "\n",
        "                    # Get the probability of taking action a given state s\n",
        "                    pi =  actions_probabilities[action]\n",
        "\n",
        "                    # Get the next state according to the selected action and given the current state\n",
        "                    next_state = environment.transition(state, action)\n",
        "\n",
        "                    # Get the value of the next state\n",
        "                    V_next_state = value_table[next_state[0], next_state[1]]\n",
        "\n",
        "                    # Get the reward of arriving in next state given the current state and the selected action\n",
        "                    reward = environment.get_reward(state, action, next_state)                   \n",
        "\n",
        "                    # Task 1.1\n",
        "                    # INSERT CODE HERE\n",
        "                    # Update the value of the current state according to equation 4.5 in the book.\n",
        "\n",
        "                    # List of acronyms and useful information\n",
        "                    # pi(a|s) -> pi\n",
        "                    # p(s', r|s, a) = 1 (once you take the action, you always land at a specific next state because it is deterministic)\n",
        "                    # vk(s') -> V_next_state\n",
        "                    # r -> reward\n",
        "                    # We are already doing the sum over the all possible actions for you!\n",
        "                    # End of list of acronyms and useful information\n",
        "\n",
        "                    # value = value + (insert rest of expression here)\n",
        "                    value = value + pi*(reward + gamma*V_next_state)\n",
        "\n",
        "                # Update our value table\n",
        "                np.put(value_table[i], j, value)\n",
        "\n",
        "                # Task 1.2\n",
        "                # INSERT CODE HERE\n",
        "                # Define the stop condition so that the algorithm stops updating the policy once the the biggest\n",
        "                # difference between the current value and the old value is smaller than the tolerance variable.\n",
        "                #delta = (insert expression here)\n",
        "                delta = max(delta , abs(value_old - value))\n",
        "\n",
        "    return value_table\n",
        "\n",
        "def value_iteration(environment, tolerance, gamma):\n",
        "    # We create a list of zeros of dimensions nxn, where n is the dimensionality of the grid\n",
        "    # Note that this could be randomly instantiated, as per the algorithm states\n",
        "    value_table = np.zeros((environment.grid_dim, environment.grid_dim)) \n",
        "\n",
        "    # Set the delta high so we enter the loop initially\n",
        "    delta = float('inf')\n",
        "\n",
        "    # We loop until the difference in values is smaller than the tolerance we define\n",
        "    while delta > tolerance:\n",
        "        delta = 0 \n",
        "        # Since our state is a grid, we loop through every combination of x and y position\n",
        "        # First is the x position\n",
        "        for i in range(environment.grid_dim):\n",
        "            # Second is the y position\n",
        "            for j in range(environment.grid_dim):\n",
        "\n",
        "                # Get the value to check the delta\n",
        "                value_old = value_table[i, j]\n",
        "                state = (i,j)\n",
        "\n",
        "                # Loop through every action\n",
        "                action_values = {}\n",
        "                for action in ACTIONS:\n",
        "                    # Get the next state according to the selected action and given the current state\n",
        "                    next_state = environment.transition(state, action)\n",
        "\n",
        "                    # Get the value of the next state\n",
        "                    V_next_state = value_table[next_state[0], next_state[1]]\n",
        "\n",
        "                    # Get the reward of arriving in next state given the current state and the selected action\n",
        "                    reward = environment.get_reward(state, action, next_state)\n",
        "\n",
        "                    # Task 2.1\n",
        "                    # INSERT CODE HERE\n",
        "                    # Update the value of the current state according to equation 4.10 in the book.\n",
        "                    # Right now we are only calculating the values for each action, when we finish this we will get the maximum value!\n",
        "                    # action_values[action] = (insert expression here)\n",
        "                    action_values[action] = reward + (gamma*V_next_state)\n",
        "                \n",
        "                # Task 2.2\n",
        "                # INSERT CODE HERE\n",
        "                # Select the highest valued state/action. Tip: action_values.values() returns a list of all values for all actions.\n",
        "                # best_val =  (insert expression here)\n",
        "                best_val = max(action_values.values())\n",
        "\n",
        "                # Update the table\n",
        "                np.put(value_table[i], j, best_val)\n",
        "\n",
        "        # Task 2.3\n",
        "        # INSERT CODE HERE\n",
        "        # Define the stop condition so that the algorithm stops updating the value once the the biggest\n",
        "        # difference between the current value and the old value is smaller than the tolerance variable.\n",
        "        #delta = (insert expression here)\n",
        "        delta = max(delta , abs(value_old - best_val))\n",
        "\n",
        "    # Now that we have all state values, we can define the optimal policy\n",
        "    policy = {}\n",
        "    # Loop through the states to assign actions\n",
        "    # Start with x position\n",
        "    for i in range(environment.grid_dim):\n",
        "        # Second is the y position\n",
        "        for j in range(environment.grid_dim):\n",
        "            state = (i,j)\n",
        "            # Get the action values\n",
        "            action_values = get_prob_weighted_action_vals(environment, state, value_table, gamma)\n",
        "            # Select the best action\n",
        "            best_act_ind = np.argmax(list(action_values.values()))\n",
        "            # Save the best action\n",
        "            policy[state] = list(action_values)[best_act_ind]\n",
        "\n",
        "    return value_table, policy\n",
        "\n",
        "# A helper function to get a list of values received for all of the actions in a state\n",
        "def get_prob_weighted_action_vals(environment, state, value_table, gamma):\n",
        "    values = {}\n",
        "    # Loop through every action\n",
        "    for action in ACTIONS:\n",
        "        value = 0\n",
        "        # Get the probability of a transition occurring\n",
        "        next_state = environment.transition(state, action)\n",
        "        prob = 1\n",
        "        # For every transition calculate the probability weighted value\n",
        "        # First get the reward\n",
        "        reward = environment.get_reward(state, action, next_state)\n",
        "        # Add the discounted next state value and multiple by the occurence probability\n",
        "        value += prob*(reward + gamma*value_table[next_state[0], next_state[1]])\n",
        "        values[action] = value\n",
        "    return values\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # Set to true to run policy evaluation, false otherwise\n",
        "    run_policy_eval = True\n",
        "    # Set to true to run value iteration, false otherwise\n",
        "    run_value_iter = True\n",
        "\n",
        "    # Create the environment and policy\n",
        "    env = gridworld()\n",
        "    policy = assignmentUniformPolicy()\n",
        "\n",
        "    if run_policy_eval:\n",
        "        print('Running Policy Evaluation')\n",
        "        value_table = policy_evaluation(env, 1e-3, policy, 0.9)\n",
        "        value_table = np.squeeze(value_table)\n",
        "        print_grid(value_table)\n",
        "        print('-----------------------------------------------------')\n",
        "    if run_value_iter:\n",
        "        print('Running Value Iteration')\n",
        "        value_table, policy = value_iteration(env, 1e-3, 0.9)\n",
        "        value_table = np.squeeze(value_table)\n",
        "        print_grid(value_table)\n",
        "        print('\\nOptimal policy after computing the optimal values')\n",
        "        print_policy(policy)\n",
        "        print('-----------------------------------------------------')\n"
      ]
    }
  ]
}